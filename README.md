### Language-Aware Visual Explanations (LAVE) ; a combination of text and visual explanation framework for image classification.

This repository presents **LAVE**, a unique explainer tool for the ImageNet dataset, combining both **SHAP** (SHapley Additive exPlanations) and **SAM** (Segment Anything Model) for visual masks, and **VLM Llava** for generating textual explanations. It allows users to generate both visual and textual explanations without requiring additional training. 

## Overview

LAVE offers an explainer for models based on the ImageNet dataset, allowing you to interpret model predictions through:

- **SHAP-based explanations** for visual and textual insights.
- **Visual masks** highlighting objects of interest in images, generated by SAM using SHAP coordinates.
- **Textual explanations** generated by VLM Llava for each visual mask.


## Architecture

<p align="center">
<img src= "https://github.com/Purushothaman-natarajan/LAVE-Explainer/blob/main/data/Architecture%20SHAP.jpg" width="800" />
</p>



## Features

1. **SHAP-Based Explanations**: Provides visual and textual explanations based on SHAP values, requiring no additional training on ImageNet.
2. **SAM Integration**: Uses SAM to generate visual masks from SHAP coordinates highlighting important regions.
3. **Textual Explanations**: VLM Llava generates language-aware, coherent textual descriptions for the visual explanations.
4. **Modular Use**: Capable of using both custom-trained models and pre-trained models directly from TorchHub.


## How to Use

### Installation

Clone the repository and install the dependencies listed in `requirements.txt`:

```bash
git clone https://github.com/Purushothaman-natarajan/LAVE-Explainer.git
cd LAVE-Explainer
pip install -r requirements.txt
```


## Explainer Script

You can use either of the following scripts based on your model type:

1. **Pre-trained Models**: Use `pre-trained_model_explainer.py` for playing with pre-trained models available on TorchHub.
   
   ```bash
   python pre-trained_model_explainer.py --model_name <pretrained_model_name> --img_path <path_to_image>
   ```

2. **Custom-Trained Models**: Use `custom_model_explainer.py` for explaining models trained using the codes in this repo.

   ```bash
   python custom_model_explainer.py --model_path <path_to_custom_model> --img_path <path_to_image>
   ```

---

## Model Training

This section covers how to use the provided scripts to train a model for custom explanations.

1. **Data Preparation**: Use `data_loader.py` to load and split your dataset into train, validation, and test sets.

   **Dataset Structure:**

      The data should follow this structure:
      
      ```sh
      ├── Dataset (Raw)
         ├── class_name_1
         │   └── *.jpg
         ├── class_name_2
         │   └── *.jpg
         ├── class_name_3
         │   └── *.jpg
         └── class_name_4
             └── *.jpg
      ```

   ```bash
   python data_loader.py --path <data_path> --target_folder <output_folder> --dim 224 --batch_size 32 --num_workers 4 --augment_data
   ```

3. **Model Training**: Train a model using `train.py` with the dataset split by `data_loader.py`.

   ```bash
   python train.py --base_model_names <comma_separated_model_names> --shape 224 --data_path <path_to_data> --log_dir <log_dir> --model_dir <model_dir> --epochs 50 --optimizer adam --learning_rate 0.001 --batch_size 32
   ```

4. **Testing**: Test your trained model using `test.py`.

   ```bash
   python test.py --data_path <path_to_test_data> --base_model_name <model_name> --model_path <path_to_model> --log_dir <log_dir>
   ```

5. **Prediction**: Predict labels on new images using `predict.py`.

   ```bash
   python predict.py --model_path <path_to_model> --img_path <path_to_image> --train_dir <path_to_train_data> --base_model_name <model_name>
   ```


## Dependencies

- Python
- SHAP
- SAM (Segment Anything Model)
- VLM Llava
- PyTorch (for pre-trained models and custom model training)
- Other dependencies listed in `requirements.txt`

#### Contact:
For any inquiries or feedback, please contact [purushothamanprt@gmail.com / c30945@srmist.edu.in].

#### Acknowledgments:
We would like to acknowledge the developers of SHAP, SAM, and VLM Llava for their invaluable open-source models, as well as our funder, DRDO, India, in the field of explainable AI.

## Citation

If you use this repository, please cite the following paper:

```
@article{natarajan2024vale,
  title={VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models},
  author={Natarajan, Purushothaman and Nambiar, Athira},
  journal={arXiv preprint arXiv:2408.12808},
  year={2024}
}
```
