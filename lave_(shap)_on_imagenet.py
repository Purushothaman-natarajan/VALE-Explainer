# -*- coding: utf-8 -*-
"""LAVE (SHAP) on ImageNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-I4cxTQ2WtwpwxgywWUv29j8vugV4aKD
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow==2.15.1
!pip install keras
!pip install scikit-learn
!pip install pandas
!pip install numpy
!pip install transformers
!pip install seaborn
!pip install shap

import tensorflow as tf
import keras
import sklearn
import pandas as pd
import numpy as np
import transformers
import seaborn as sns
import shap

print("TensorFlow version:", tf.__version__)
print("Keras version:", keras.__version__)
print("Scikit-learn version:", sklearn.__version__)
print("Pandas version:", pd.__version__)
print("Numpy version:", np.__version__)
print("Transformers version:", transformers.__version__)
print("Seaborn version:", sns.__version__)
print("SHAP version:", shap.__version__)

"""**Load the Pre-trained Model and its decoders**"""

import cv2
import matplotlib.pyplot as plt
import numpy as np

# Load the image
image_path = "/content/drive/MyDrive/Backups : SHAP -Image-to-text /SHAP Image-to-text/SHAP for image-to-text model/Test_Images_2/Papillion dog.jpg"

# Define the directory containing the image
custom_image = cv2.imread(image_path)

# Preprocess the image
custom_image = cv2.cvtColor(custom_image, cv2.COLOR_BGR2RGB)  # Convert to RGB format
image = cv2.resize(custom_image, (224, 224))  # Resize to fit model input size

# Plot reshaped image
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis('on')
plt.show()

from tensorflow.keras.applications import DenseNet121, imagenet_utils
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import img_to_array, load_img
import numpy as np
import PIL
from PIL import Image

# Load the pre-trained DenseNet121 model with ImageNet weights
model = DenseNet121(weights="imagenet")

# Define the target image size (DenseNet121 expects 224x224)
target_size = (224, 224)

# Load the image
img = load_img(image_path, target_size=target_size)

# Convert the image to a NumPy array
img_array = img_to_array(img)

# Preprocess the image (normalize pixel values between 0 and 1)
img_array = img_array / 255.0

# Expand the dimensions of the image to match the model's input format
img_array = np.expand_dims(img_array, axis=0)

# Predict the class probabilities
predictions = model.predict(img_array)

# Decode the predictions to obtain the top 3 class labels and probabilities
decoded_predictions = imagenet_utils.decode_predictions(predictions, top=3)[0]

print("Top 3 predictions:")
for i, (imagenet_id, label, score) in enumerate(decoded_predictions):
    print(f"{i + 1}: {label} ({score:.2f})")
# Decode the predictions to obtain the class labels
decoded_predictions = imagenet_utils.decode_predictions(predictions)

"""**Initialize the Explainer - SHAP**"""

# SHAP Explainer
import json
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.densenet import preprocess_input
import shap

def f(X):
    tmp = X.copy()
    preprocess_input(tmp)
    return model(tmp)

X, y = shap.datasets.imagenet50()

# load the ImageNet class names as a vectorized mapping function from ids to names
url = "https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
with open(shap.datasets.cache(url)) as file:
    class_names = [v[1] for v in json.load(file).values()]

# define a masker that is used to mask out partitions of the input image, this one uses a blurred background
masker = shap.maskers.Image("inpaint_telea", X[0].shape)

# By default the Partition explainer is used for all  partition explainer
explainer = shap.Explainer(f, masker, output_names=class_names)

import shap
import matplotlib.pyplot as plt
import cv2
import numpy as np

# Assuming 'decoded_predictions' contains the top 3 predictions
top_predictions = decoded_predictions[0]

# Extracting the labels from the predictions
labels = [label for _, label, _ in top_predictions]

print(labels)

# Define the directory containing the image
custom_image = cv2.imread(image_path)

# Preprocess the image
custom_image = cv2.cvtColor(custom_image, cv2.COLOR_BGR2RGB)  # Convert to RGB format
custom_image = cv2.resize(custom_image, (224, 224))  # Resize to fit model input size
custom_image = custom_image.astype(np.float32) / 255.0  # Normalize pixel values

# Get SHAP values
shap_values = explainer(
    custom_image.reshape(1, 224, 224, 3), max_evals=1000, batch_size=50, outputs=shap.Explanation.argsort.flip[:1]
)

# Plot SHAP values
shap.image_plot(shap_values[0], custom_image, labels=labels)
plt.show()

# Extract the co-ordinates with maximum SHAP values
shap_values_abs = np.abs(shap_values[0].values)
top_indices = shap_values_abs.argsort(axis=None)[::-1]  # Get indices of all regions sorted by SHAP value (descending)
top_coordinates = np.unravel_index(top_indices, shap_values_abs.shape)  # Convert indices to coordinates

# Track selected scores and coordinates to ensure uniqueness
selected_scores = set()
selected_coordinates = []

print("Top SHAP Scores and their Coordinates in Original Image:")
for i in range(len(top_indices)):
    coord = tuple(coord[i] for coord in top_coordinates)  # Ensure coordinates are in tuple form
    score = shap_values_abs[coord]
    if score not in selected_scores:  # Check if score is unique
        selected_scores.add(score)
        selected_coordinates.append((score, coord))

# Print and store the unique scores and coordinates
for i, (score, coord) in enumerate(selected_coordinates[:10]):
    print(f"Top {i+1}: Score={score}, Coordinates={coord}")

# Extracting y and x coordinates from the first channel (index 0) only
coordinates = [(coord[1][1], coord[1][0]) for coord in selected_coordinates if coord[1][2] == 2]

print(f'Top 3 SHAP Value Coordinates: {coordinates[:3]}')

"""**Intialize Segment Anything Model (SAM)**"""

!pip install git+https://github.com/facebookresearch/segment-anything.git

import numpy as np
import matplotlib.pyplot as plt

def show_mask(mask, ax, random_color=False):
    """
    Display a mask on the given axis.

    Parameters:
    - mask: numpy array, the mask to display.
    - ax: matplotlib axis object, the axis to display the mask on.
    - random_color: bool, whether to use a random color for the mask or not.
    """
    if random_color:
        # Generate a random color with transparency
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        # Use a default color for the mask
        color = np.array([30/255, 144/255, 255/255, 0.6])

    # Get the height and width of the mask
    h, w = mask.shape[-2:]

    # Reshape the mask and multiply with color for visualization
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)

    # Display the mask on the axis
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    """
    Display points on the given axis.

    Parameters:
    - coords: numpy array, coordinates of points.
    - labels: numpy array, corresponding labels for each point.
    - ax: matplotlib axis object, the axis to display the points on.
    - marker_size: int, size of the marker for each point.
    """
    # Separate positive and negative points based on labels
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]

    # Plot positive points in green
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

    # Plot negative points in red
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

def show_box(box, ax):
    """
    Display a bounding box on the given axis.

    Parameters:
    - box: list or array, containing coordinates of the box [x0, y0, x1, y1].
    - ax: matplotlib axis object, the axis to display the box on.
    """
    # Extract box coordinates
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]

    # Add rectangle patch to the axis
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))

# Load and set the SAM predictor from the local directory

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor

sam_checkpoint = "/content/drive/MyDrive/SHAP for image-to-text model (1)/SAM/sam_vit_h_4b8939.pth"
model_type = "vit_h"

#device = "cuda"
device = "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

# Fix the co-ordinates and the number of inputs (prompts) to the SAM model.

converted_array = np.array(coordinates) # convert the co-ordinates to a array

input_point = converted_array[:1] # set the prompts
print(input_point)

input_label = np.array([1]) # set the number of prompts
print(input_label)

# Load the image and process it for SAM.

import cv2
import matplotlib.pyplot as plt
import numpy as np

# Define the directory containing the image
custom_image = cv2.imread(image_path)

# Preprocess the image
custom_image = cv2.cvtColor(custom_image, cv2.COLOR_BGR2RGB)  # Convert to RGB format
image = cv2.resize(custom_image, (224, 224))  # Resize to fit model input size

# Set the predictor to the input image

predictor.set_image(image)

# Visualize the co-ordinates with maximum SHAP values.

plt.figure(figsize=(10,10))
plt.imshow(image)
show_points(input_point, input_label, plt.gca())
plt.axis('on')
plt.show()

# Predict the segments using SAM with prompt (co-ordinates with maximum SHAP value)

masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True,
)

# Visualize the results

for i, (mask, score) in enumerate(zip(masks, scores)):
    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    show_mask(mask, plt.gca())
    show_points(input_point, input_label, plt.gca())
    plt.title(f"Mask {i+1}, Score: {score:.3f}", fontsize=18)
    plt.axis('off')
    plt.show()

import cv2

# Find index of mask with highest score
index_of_highest_score = np.argmax(scores)


# Save the image in the masked region and explain further.

import cv2

# Find index of mask with highest score
mask_with_highest_score = masks[-1]

# Apply the mask to the original image to extract content inside the mask
image_with_mask = image.copy()  # Assuming 'image' is the original image
image_with_mask[mask_with_highest_score == 0] = [0, 0, 0]  # Set pixels outside mask to black

# Save the image with only content inside the mask overlaid
content_inside_mask_on_original_image_path = "content_inside_mask_on_original_image.jpg"
cv2.imwrite(content_inside_mask_on_original_image_path, cv2.cvtColor(image_with_mask, cv2.COLOR_RGB2BGR))

# Now you have the original image with only content inside the mask overlaid saved as an image
print("Original image with content inside the mask overlaid saved at:", content_inside_mask_on_original_image_path)

"""**Initialize the VLM ; LLaVA**"""

!pip install git+https://github.com/DLCV-BUAA/TinyLLaVABench.git

device = "cuda"

from tinyllava.model.builder import load_pretrained_model
from tinyllava.mm_utils import get_model_name_from_path
from tinyllava.eval.run_tiny_llava import eval_model
model_path = "bczhou/TinyLLaVA-3.1B"
tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path)
)

import cv2
import matplotlib.pyplot as plt

image_file = "./content_inside_mask_on_original_image.jpg"
custom_image = cv2.imread(image_file)

# Preprocess the image
custom_image = cv2.cvtColor(custom_image, cv2.COLOR_BGR2RGB)  # Convert to RGB format
image = cv2.resize(custom_image, (224, 224))  # Resize to fit model input size

# Plot reshaped image
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis('on')
plt.show()

{top_3_labels[0]} # replace with the label 'Actual Predicted Label' ;

from tinyllava.model.builder import load_pretrained_model
from tinyllava.mm_utils import get_model_name_from_path
from tinyllava.eval.run_tiny_llava import eval_model
model_path = "bczhou/TinyLLaVA-3.1B"
prompt = f"Explain the object in the image: 'papillon'?"
image_file = "./content_inside_mask_on_original_image.jpg"

args = type('Args', (), {
    "model_path": model_path,
    "model_base": None,
    "model_name": get_model_name_from_path(model_path),
    "query": prompt,
    "conv_mode": "phi",
    "image_file": image_file,
    "sep": ",",
    "temperature": 0,
    "top_p": 1,
    "num_beams": 1,
    "max_new_tokens": 1024
})()
eval_model(args)